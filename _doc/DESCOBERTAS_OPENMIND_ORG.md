# üîç Descobertas - OpenMind.org API

Baseado na documenta√ß√£o: https://docs.openmind.org/api-reference/introduction

---

## ‚úÖ Informa√ß√µes Encontradas

### 1. Autentica√ß√£o

**Formato:**
```http
x-api-key: YOUR_API_KEY
# ou
Authorization: Bearer YOUR_API_KEY
```

**Sua chave:** `om1_live_7d4102a1bf72cc497d7651beb6a98292764b1f77df947c82d086506038ea6b9921efb9d9833045d1`

---

### 2. Modelos Dispon√≠veis

Eles t√™m v√°rios modelos, incluindo:

#### Modelos de Vis√£o (VLM):
- **qwen2.5-vl-72b-instruct** (Near AI)
  - Input: $0.59 por 1M tokens
  - Output: $0.59 por 1M tokens
  - **Perfeito para an√°lise de imagens!**

#### Outros Modelos √öteis:
- OpenAI GPT-4o: $2.5/$10
- OpenAI GPT-4o-mini: $0.15/$0.6
- Gemini 2.0 Flash Exp: $0.15/$0.30

---

### 3. Endpoints

Na documenta√ß√£o, vejo refer√™ncias a:
- **VILA VLM** - Modelo de vis√£o!
- LLMs - Modelos de linguagem
- Outros servi√ßos

---

## üîç Pr√≥ximo Passo

Preciso encontrar:
1. **URL base da API** (provavelmente `https://api.openmind.org`)
2. **Endpoint espec√≠fico para VILA VLM** ou an√°lise de imagens
3. **Formato da requisi√ß√£o** para an√°lise de imagens

---

## üí° Pr√≥xima A√ß√£o

Vou buscar mais informa√ß√µes sobre o endpoint VILA VLM ou como usar modelos de vis√£o no OpenMind.org.

Ou voc√™ pode me passar:
- Link para documenta√ß√£o do VILA VLM
- Ou exemplo de como usar an√°lise de imagens

---

**J√° temos o in√≠cio! Agora preciso do endpoint espec√≠fico!** üöÄ
